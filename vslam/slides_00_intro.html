<html>
<head>
<style>
body {
   font-family: monospace;
    font-size:large;
    text-align: center;
    background: linear-gradient(141deg,
        white,
        white,
        #e0fbff,
        #ffaaaa,
        #3157af
        );
}
div {
    text-align: justify;
    max-width: 500px;
    display: inline-block;
}
img {
    max-width: 500px;
}
iframe {
    width: 500px;
    height: 280px;
}
h3 {
    text-align: center;
}
.wide {
    max-width: 750px;
}

#how_it_works_pics {
    max-width: 750px;
}
</style>
<title>Mike Garmulewicz - gf4c3 - gf4c3k1la </title>
</head>
<body>


<div>
<img src="../img/vslam/logo.png">
</div>
<br><br>

<h2>VSLAM course - Part 0x0 - Intro</h2>
<br><br>

<div>
<h3> Agenda of this lecture: </h3>
<ol>
  <li> What is this course?
  <li> References
  <li> What's VSLAM?
  <li> Draft of the system
</ol>
</div>
<br><br>


<div>
<h3> What is this course? </h3>
<ol>
  <li> Have fun!
  <br>
  <li> Tries to maximize accessibility in some sense:
    <ul>
      <li> One <b>simple</b> approach from many possible, but complete.
      <li> No black boxes (see all the way through abstraction).
    </ul>
  <br>
  <li> To hit this goal, we take opinionated choices.
    <ul>
      <li> <b>readability</b> > <i>performance</i> - in real life, you need opposite.
      <li> simulated data
    </ul>
  <br>

  <li> Don't just read it, 
    <a href="https://letmegooglethat.com/?q=Don%27t+just+read+it%2C+fight+it!">
  fight it!</a>
  <br>
</ol>
</div>
<br><br>



<div>
<h3> References, tuned to your goals </h3>
This course is based on this book:
</div>
<br><br>
<br>

<div>
<img src="../img/vslam/intro_to_vslam.jpg" style="max-width:200px">
</div>
<br><br>

<div>
<ul>
<li> This book is 
<a href="https://github.com/gaoxiang12/slambook-en/blob/master/slambook-en.pdf">
open source and pdf is hosted on github.</a> 
<li> The practical C++ code presented in this book is
<a href="https://github.com/gaoxiang12/slambook-en">
also open source and available for free on github</a> 
<li> I am extermely grateful to authors of this book.
</div>
<br><br>


<div>
Our python code that we will in this course is here:
</div>
<br><br>


<div>
<a href="https://github.com/ghostFaceKillah/vslam">
https://github.com/ghostFaceKillah/vslam</a>
</div>
<br><br>




<div>
What's VSLAM:
<li>  SLAM - simultaneous visualization and mapping, but based on vision
<li> given cameras (usually one or two, but could be more) and IMU (accelometer) figure out:
    <ul>  
      <li> where you are and
      <li> how the environment around you looks like. "Where are the walls and how far are you from them"
    </ul>  
  <li> the biggest practical difference between VSLAM and SLAM is that usually SLAM uses 2D or 3D  LiDaR
  <li> LiDaR (which are laser distance meters)
    gives you ~20 scans per second in 2d and 3d about where you are

  <li> 

  - don't buy wholesale popular narratives "Lidar is an expensive crutch. Humans can navigate from vision perfectly well".
    it is partially true, but the sitation is dynamic.
    LidarS are becoming cheaper at an exponential pace. Cameras have their own complexities and blindspots.
    Cameras can be hard to integrate and surprisingly expensive if you are going for quality.

</div>
<br> <br> 

<div>
<ul>
  <li>How does the system work?
  <ul>
    <li>The robot has two cameras.</li>
    <li>The robot goes thru env in timesteps and each time we get picture from left eye and right eye.</li>
    <li>We could do one of 2 things at the given timestep:
    <ul>
      <li>Firstly, it uses information from two cameras to estimate depth of the points it sees.
        <ul>
          <li>we match up points from left image to points in the right image.</li>
          <li>we know how far apart are the eyes of the robot, and based on this we could figure out how far away from us are the things.</li>
          <li>Then, the robot remembers this information and call this reference frame "keyframe".
            <ul>
              <li>so keyframe is a picture from two cameras, where we have estimated depth of points</li>
              <li>The special thing about the keyframe is that it has a bunch of 3d points attached to it.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Then, as the robot moves on, we could compare the new information from left eye in reference keyframe to information that the left eye currently sees.
        <ul>
          <li>It tries to figure out which elements of the image are the same elements as it has seen before and based on their movement, it figures out how much it has moved.</li>
          <li>As it gets away from the keyframe, the shared elements between
               the keyframes current left eye image and past left eye image might get more
               rare. We have moved on so we see other stuff. So we need to make a new keyframe
               every now and then</li>
        </ul>
      </li>
    </ul>
    </li>
  </ul>
  </li>
</ul>




</div>
<br> <br> 



<div>
<ul>
    <li>What things we will need to understand
        <ul>
            <li>Ways to express poses mathematically: out of couple of possibilities, we will choose SO(3) SE(3)</li>
            <li>coordinate systems & coordinate transormations
                <ul>
                    <li>if this object is at position xyz when measured wrt to left eye, where is it wrt to right eye?</li>
                </ul>
            </li>
            <li>pinhole camera system / equations</li>
            <li>rendering</li>
            <li>feature matching</li>
            <li>depth estimation</li>
            <li>gauss newton</li>
            <li>3d pnp via</li>
            <li>putting it together - the actual VSLAM</li>
        </ul>
    </li>
</ul>
</div>
<br> <br>





<div>
More specialized refrences:
<ul>
  <li> steepest possible learning curve:
  <ul>
  <li> go to ORB-SLAM3 source code and paper and start reading. 
  </ul>
  <br>
  <li>
    There's a book and library.
    <ul>
      <li> <a href="http://www.cs.cmu.edu/~kaess/pub/Dellaert17fnt.pdf">
        Factor Graphs for Robot Perception</a>
        - principled and mathematically grounded introduction into factor-based SLAM backend.
        We are very grateful to the authors for free pdf. Thank you!
      <br> 
      <br> 
      <li> <a href="https://github.com/borglab/gtsam">
      The library GTSAM</a>
      same authors (+ open source community, lab members and grad students)


    </ul>

    - Probabilistic robotics by Thrun

  - for SE(3), SO(3) there's many:
    - Barfoot
    - Naive Lie Theory

  SOTA
  - deep learning paper
    
</ul>
</div>
<br> <br> 
