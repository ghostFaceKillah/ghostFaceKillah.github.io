<html>
<head>
<style>
body {
   font-family: monospace;
    font-size:large;
    text-align: center;
    background: linear-gradient(141deg,
        white,
        white,
        #e0fbff,
        #ffaaaa,
        #3157af
        );
}
div {
    text-align: justify;
    max-width: 500px;
    display: inline-block;
}
img {
    max-width: 500px;
}
iframe {
    width: 500px;
    height: 280px;
}
h3 {
    text-align: center;
}
.wide {
    max-width: 750px;
}

#how_it_works_pics {
    max-width: 750px;
}
</style>
<title>Mike Garmulewicz - gf4c3 - gf4c3k1la </title>
</head>
<body>


<div>
<img src="../img/vslam/logo.png">
</div>
<br><br>

<h2>VSLAM course - Part 0x0 - Intro</h2>
<br><br>

<div>
<h3> Agenda of this lecture: </h3>
<ol>
  <li> What is this course?
  <li> References
  <li> What's VSLAM?
  <li> Draft of the system
</ol>
</div>
<br><br>


<div>
<h3> What is this course? </h3>
<ol>
  <li> Have fun!
  <br>
  <li> Tries to maximize accessibility in some sense:
    <ul>
      <li> One <b>simple</b> approach from many possible, but complete.
      <li> No black boxes (see all the way through abstraction).
    </ul>
  <br>
  <li> To hit this goal, we take opinionated choices.
    <ul>
      <li> <b>readability</b> > <i>performance</i> - in real life, you need opposite.
      <li> simulated data
    </ul>
  <br>

  <li> Don't just read it, 
    <a href="https://letmegooglethat.com/?q=Don%27t+just+read+it%2C+fight+it!">
  fight it!</a>
  <br>
</ol>
</div>
<br><br>



<div>
<h3> Opening references </h3>
The course is roughly based on this book:
</div>
<br><br>
<br>

<div>
<img src="../img/vslam/intro_to_vslam.jpg" style="max-width:200px">
</div>
<br><br>

<div>
<ul>
<li> This book is 
  <a href="https://github.com/gaoxiang12/slambook-en/blob/master/slambook-en.pdf">
  open source and pdf is hosted on github.</a> 
<li> The practical C++ code presented in this book is
  <a href="https://github.com/gaoxiang12/slambook-en">
  also open source and available for free on github</a> 
<li> I am extermely grateful to authors of this book.
</ul>
<br><br>

</div>
<br><br>


<div>
Our python code that we will in this course is here:
</div>
<br><br>


<div>
<a href="https://github.com/ghostFaceKillah/vslam">
https://github.com/ghostFaceKillah/vslam</a>
</div>
<br><br>




<div>
<h3> What's VSLAM? </h3>
<b>SLAM</b> - simultaneous visualization and mapping based on vision.
<br><br>
<li> given cameras (usually one or two, but could be more) and IMU (accelometer)
   we want to figure out:
    <ul>  
      <li> Where are we?
      <li> How does the environment around us look like?
      <li> "Where are the walls and how far are we from them"?
    </ul>  
  <li> SLAM vs VSLAM: the biggest practical difference is that SLAM uses 2D or 3D LiDaR <br><br>
  <li> LiDaR (which are laser distance meters)
    gives us ~20 scans per second in 2d or 3d  <br><br>

  <li> Some say "Lidar is an expensive crutch. Humans can navigate from vision perfectly well"
    <ul>
    <li> This is only partially true. 
    <li> The sitation is dynamic.
    <li> Lidars are becoming exponentially cheaper.
    <li> Cameras have their own complexities and blindspots.  
    <li> Cameras can be hard to integrate and surprisingly expensive if you are going for quality.
    </ul>
</div>
<br> <br> 

<div>
<h3> How does the system work?</h3>
  <ul>
    <li>The robot has two cameras.
    <li>The robot goes through the environment in timesteps 
  </ul>
</div>
<br> <br> 


<div>
<img src="../img/vslam/slides_00_robot_goes_through_env.jpg" style="max-width:400px">
</div>
<br><br>

<div>
<ul>
  <li> At each timestep we get a picture from the left eye and right eye.
  <li>We could do one of 2 things at the given timestep:
    <ul>
    <li> Establish "keyframe" reference location. We will relate our later location in 
         the world comparing to this location.
    <li> Compute our pose in relation to this previously saved keyframe
    </ul>
    </ul>
</ul>
</div>
<br><br>


<div>

First, by comparing image from two eyes we could estimate depth of things we see.
    <ul>
    <li>we match up points from left image to points in the right image.</li>
    <li>we know how far apart are the eyes of the robot, 
       and based on this we could figure out how far away from us are the things.</li>
    </ul>
</ul>
</div>
<br><br>



<div>
<img src="../img/vslam/slides_00_birdeye-view.jpg" style="max-width:400px">
</div>
<br><br>


<div>
<ul>
    <ul>
      <li> We remember depth information and call this reference 
            3d-from-two-eyes picture the "keyframe".
      </li>
    </ul>
  </li>
</ul>
</div>
<br><br>

<div>
  Then, as the robot moves on, we compare the new information 
  to the reference keyframe.
</div>
<br><br>

<div>
<img src="../img/vslam/slides_00_comparing_keyframes.jpeg" style="max-width:400px">
</div>
<br><br>


<div>
    <ul>
      <li> We try to figure out which points in the new image are the same elements 
          that we have seen before. Based on their movement, we figure out how much we
           have moved.
      <li>As it gets away from the keyframe, the shared elements between
           the keyframes current left eye image and past left eye image might get more
           rare. We have moved on so we see other stuff. So we need to make a new keyframe
           every now and then
    </ul>
</div>
<br> <br> 


<div>
<h3> Plan of the lecture: </h3>
<ul>
    <li> Expressing poses of robot as SO(3) and SE(3).
    <li>coordinate systems & coordinate transormations
        <ul>
        <li>if this object is at position xyz when measured wrt to left eye, where is it wrt to right eye?</li>
        </ul>
    </li>
    <li>Pinhole camera system / equations</li>
      <ul>
      <li> if object is at such and such position wrt the robot, where is it in the picture?
      </ul>
    <!-- <li>rendering</li> -->
    <li>Feature matching</li>
    <li>Depth estimation</li>
    <li>Gauss Newton optimization & PNP (Point n Perspective)
      <ul><li> If we know which correspondence of objects in the current frame 
        and keyframe, how do we compute current pose wrt to keyframe?
      </ul>
    <li>putting it together - the actual VSLAM</li>
</ul>
</div>
<br> <br>





<div>
<h3>More specialized refrences</h3>
<ul>

  <li> Steepest possible learning curve:
    <ul>
    <li> go to <a href="https://github.com/UZ-SLAMLab/ORB_SLAM3">
ORB-SLAM3 source code</a> and <a href="https://ieeexplore.ieee.org/document/9440682">paper</a> and start reading. 
    </ul>
  <br>
  <li> To get stuff done in non-deep-learning way in real life, 
    there's a widely-used and practicioner-recommended book and C++ library.
    <ul>
      <li> <a href="http://www.cs.cmu.edu/~kaess/pub/Dellaert17fnt.pdf">
        Factor Graphs for Robot Perception</a>
        - principled and mathematically grounded introduction into factor-based SLAM backend.
        We are very grateful to the authors for free pdf. Thank you!
      <br> 
      <br> 
      <li> <a href="https://github.com/borglab/gtsam">
      The library GTSAM</a>
      same authors (+ open source community, lab members and grad students)
    </ul>
  <li>
    There's a bunch of classic books:
    <ul>
      <li> Gentle, cute and classic introduction to mapping as Bayesian filtering:
        <a href="http://www.probabilistic-robotics.org"> Probabilistic robotics</a>.
      <li> For SE(3), SO(3) there's 2 classic textbooks:
        <ul>
          <li> In the context of robotics
            <a href="http://asrl.utias.utoronto.ca/~tdb/bib/barfoot_ser17.pdf">
            State Estimation for Robotics"</a>
          <li> In the context of math 
            <a href="https://link.springer.com/book/10.1007/978-0-387-78214-0">
            Naive Lie Theory</a>
          <li> For math lovers, robot haters:
            <a href="https://math.berkeley.edu/~jchaidez/materials/reu/lee_smooth_manifolds.pdf">
            Introduction to Smooth Manifolds</a>
        </ul>
    </ul>
</ul>
</div>
<br> <br> 
