*h2*
How to build a small self driving car?

My recent side project is a building small self driving car with 
my friend Filip.  See the code on 
<a href="https://github.com/ghostFaceKillah/mini-self-driving-car.git">
my github</a>.

There is fair amount of folks doing similar projects, there are even people who
race these things.  If it looks interesting, you could do it too - it is easier than 
ever and I'm sure you'd learn a lot.

*wide-div*
<img src="img/car/car_works_1.gif">
<img src="img/car/car_works_2.gif">
<img src="img/car/car_works_3.gif">



*h4*
* Idea *

The car works in the following way:
<ul>
  <li>
    On board of the car there is forward facing <b>RasPi camera</b>.
  <li>
    The <b> image stream</b> is sent to GPU computer, 
    which streams <b>steering commands</b> back:
    "go forward", "stop", "turn left", etc.
  <li>
    Steering commands are issued in one of two ways:
    <ul>
      <li>
        <b>Human input</b>, where you steer the car in racing video game fashion.
      </li>
      <li>
        <b> Self-driving algorithm</b>, which tries to guess what would a human do 
        based on current video frame.
      </li>
    </ul>
  </li>
</ul>

Below diagaram visualizes the flow of data:

*img*
img/car/how_it_works.jpg

To enable all of this, pieces of various software infrastructure and hardware
are necessary.

On the GPU server side we need:
<ul>
<li> <a href="https://github.com/ghostFaceKillah/mini-self-driving-car/blob/master/src/pc/image_getter.py">code</a> 
     that receives images from RasPi
<li> <a href="https://github.com/ghostFaceKillah/mini-self-driving-car/blob/master/src/pc/keyboard_state_sender.py">code</a> 
     that sends current steering state to the car
<li> <a href="https://github.com/ghostFaceKillah/mini-self-driving-car/blob/master/src/pc/neural_network_computer.py">code</a> 
     that executes neural network to obtain autonomous steering
<li> <a href="https://github.com/ghostFaceKillah/mini-self-driving-car/blob/master/src/pc/pygame_driver.py">code</a> 
     that displays user the "steering game" and collects steering
<li> <a href="https://github.com/ghostFaceKillah/mini-self-driving-car/blob/master/src/pc/state_saver.py">code</a> 
     that collects human input pairs and saves them to the drive
</ul>
Each of the above is <b>multiprocessing.Process</b> and they share a state
which is <b>multiprocessing.Namespace</b>.
On the RasPi side we need:
<ul>
<li> <a href="https://github.com/ghostFaceKillah/mini-self-driving-car/blob/master/src/raspi/steering.py">code</a> that receives steering commands and executes them
<li> <a href="https://github.com/ghostFaceKillah/mini-self-driving-car/blob/master/src/raspi/video.py">code</a> that captures video stream and puts it on the wire
</ul>


On the hardware side there are following elements:
<ul>
<li> Raspberry Pi
<li> RasPi battery pack
<li> RasPi compatibile camera
<li> RC Car
<li> RC car engines battery pack
<li> Engine controller to control RC car
<li> Wifi interface (turns out it needs to be strong)
<li> Ways to keep all of this together
</ul>

In the text below I will explain how we figured out
how to combine all of them into a small self driving car,
and hopefully convey that it was a lot of fun.
And perhaps we have even learned a little.


*h4* 
* Before the first car *

The whole story started when I finally had time to learn some electronics.
I have always wanted to understand it, as it delivers such exceptional
amounts of value and - in my book - is as close to magic as it gets.

There are so many <a href="https://artofelectronics.net">cool books about electonics</a>,
<a href="http://lcamtuf.coredump.cx/electronics/">online write-ups</a>,
<a href="https://www.youtube.com/user/eaterbc">youtube videos</a>,
<a href="http://www.falstad.com/circuit/">electronics simulators</a>,
but what I found most practical for the car project was this book targetted to young teens:
<a href="https://www.amazon.com/Make-Electronics-Discovery-Charles-Platt/dp/0596153740">

*img*
img/car/book-make-electronics.jpg

</a>
It is so good:
<ul>
<li> has light-hearted vibe about it,
<li> teaches you the practical basics via small projects:
  <ul>
  <li>how to solder,
  <li> how to use multimeter,
  </ul>
<li> is exceptionally easy-to-read.
</ul>
It lacks the proper theory build-up, so you will have to read up on that
somewhere else if you are interested, but for the car project it should be
sufficient.  After some lazy evenings with soldering iron I was able to reverse
engineer and debug basic electronic circuits such as this "astable
multivibrator", to use the proper name:

*img*
img/car/blinkenlights.gif


*h3*
* The first car era *

Armed with knowledge of rudiments of electronics, it was time to reverse
engineer how an RC car works.  As I generally had little idea what I was doing,
I have decided to get cheapest one I could find so I wouldn't be too wasteful
if I broke it while disassembling it.  The cheapest one I could find was this
little marvel:

*img*
img/car/vanilla_green.jpg

which I have bought in Dutch bargain store Action.nl, new, for below 5 euro.

Deconstructing it, you just cannot help wondering how 
 simple and elegantly designed it is. 
It truly amazes me that for under 5 Euro per unit
people are able to design, fabricate, package, ship 
and sell this toy.
It is possible mostly due to amusingly "to the point", "brutalist"
design, where most of elements are right on their tolerances
and wanted effects are achieved in the simplest possible way.
This approach is best showcased by the design of the front wheels
steering system, which I will describe below.

Coming back to the self-driving project, when we pop the body
of the frame, we see this:

*img*
img/car/green_basic_deconstruct.jpg

Luckily for our project, 
this construction can be adapted to our needs in a straightforward fashion.
In the center we see the logic PCB.
There are three pairs of red & black wires attached to the PCB:
<ul>
<li> a pair feeding power from the battery pack
<li> another pair sending power to the back motor, which movese car forward and backward
<li> one more pair sending power to the front wheel steering system
</ul>

When logic board receives radio inputs, it (more-or-less) connects
the battery wires with the engine wires and the car moves.

*img*
img/car/green/green_pcb_closeup.jpg

You can see for yourself - if you directly connect the pair from the battery to
the pair coming from one of engines (front or back engine), you see that the
wheels move in wanted directions.  So if we substitute the current logic board
with our custom RasPi based logic board that controls connection between
battery and engines based on instructions that we control by code, we would be
able to drive-by-code.

*h4* 
Warning!

As Raspberry Pi has some General Purpose Input/Output (GPIO) pins
it is tempting to try to hook up the engine directly to RasPi.
This is a very bad idea, which will destroy your RasPi, by running too much power
through it.

Thus, we need to implement a hardware engine controller which based
on signals from Raspi will drive the motors.
You can buy a ready motor controller or be thrifty, and implement own.
As conceptually the wanted mechanism of action sounds like transistor - 
"based on small electricty turn on flow of bigger electricty" -
I have decided to build my own engine controller.
Following
<a href="https://business.tutsplus.com/tutorials/controlling-dc-motors-using-python-with-a-raspberry-pi--cms-20051">
this tutorial
</a>
produced an engine controller based on L293D chip.

*img*
img/car/green_engine_controller.jpg

The tutorial worked - at least partially. I was able to steer the front wheels
and move the car forward, but there was no reverse gear.

*iframe*
https://drive.google.com/file/d/0Bw2eHK8Zx0mLZ0ZxaVQ3UDdhNGM/preview

Turns out the method shown in the above tutorial couldn't spin second engine
in the reverse direction.
After consulting the 
<a href="http://www.ti.com/lit/ds/symlink/l293.pdf">
chip's factsheet
</a>
I have come up with another way of hooking up L293D, which fixed the bug. 
Now I could control car completely from Python code running on RasPi.

Now, as I had a car that can be controlled by keyboard attached to RasPi, it
was time to make the control remote. I have connected a basic small WiFi dongle
to Raspberry and implemented a pair of Python scripts:
<ul>
<li> Server-side script capturing keyboard events and sending control
events based on them to RasPi,
<li> RasPi-side script listening for those events on TCP socket and based on them
steering the L293D and in turn, the car.
</ul>


*h4*
* Steering mechanism in first car *

Now I will describe how steering is implemented mechanically 
in the first RC car we have been working with. 

Let me remark here that I am impressed by how simple this mechanism is.
Perhaps the reason I am so excited about it is because I have only a vague
idea about how mechanical design works, so I am easy to impress, and perhaps
stuff like this gets done every day, who knows.  But I see an element of playful
master in it and I can't help feeling happy about it.


The mechanism is hidden if you look from the front

*img*
img/car/steering/green_iso_front_view.jpg

and similarily remains hidden if you look at it from the top.

*img*
img/car/steering/green_top_view.jpg

It is only after removing the top cover that we see the following mechanism
(I have removed all the plastic elements to help visibility):

*img*
img/car/steering/green_side_front_engine.jpg

Seen from the top:

*img*
img/car/steering/green_engine_top.jpg

And here comes the trick:

On top of shaft there is a pinion. The pinion is just slid on shaft, without
any kind of glue.  The size and tightness of the pinion are chosen in in a way
that when the engine starts moving, the friction between shaft and pinion is
just enough to move the half-circle rack.

*img*
img/car/steering/green_front_engine_detail.jpg


But when the half-circle rack hits
the maximum extension point, the friction breaks and the shaft spins around
inside pinion.  Is there a cheaper trick (speaking both symbolically and
literally)?  How ingenious!

*img*
img/car/steering/green_steering_mechanism_action.gif

Below the semicircular rack there is a spring that pulls the wheels back
to neutral position:

*img*
img/car/steering/green_top_view_wheels_coming_off.jpg

You can see it better in this close-up:

*img*
img/car/steering/green_steering_insides_2.jpg

A small plastic element is connected to wheels. It has a small peg
that sits in the middle of the spring, so the spring an act on it:

*img*
img/car/steering/green_detached_wheels.jpg

So as you can see just a collection of as simple as possible 
elements each doing its job well. Everything is super cheap,
but the materials and their shapes are chosen in a smart
way that makes it all work together well. So cool!

That wraps up the description of the "brutalist"
steering mechanism.  Back to self driving!





*h4*
* Hardware * 

When we had already the "drive by keyboard attached to RasPi by cable"
capability, we wanted to test remote control of the car.
In order to do that, we needed to power the RPi.
Not sure about how much power it needs, we have bought a pretty big power
bank for mobile phone with electric charge of 10.000 mAh, weighting around 400 g.

I am huge fan of American automotive show 
<a href="https://www.youtube.com/watch?v=t8zYpmoV0qE&list=PL12C0C916CECEA3BC">
Roadkill</a>,
in which two guys, mostly by themselves, fix extremely clapped out classic
American cars, usually on the road / in WalMart parking lot / in the junkyard.
Typically they take some rotten engine-less chassis and put in a
cheapest-they-can-find V8 into them.  Needless to say, they are masters of
using zip-ties.

Inspired by Freiburger and Finnegan experiences, I have relied on ziptie engineering
in order to combine the collection of parts into one coherrent vehicle
that will move together. The end product looked like this:

*img* 
img/car/green/green_heavy_iso_03.jpg

*img* 
img/car/green/green_heavy_iso_02.jpg

*img* 
img/car/green_ready_front.jpg

*img* 
img/car/green_ready_top.jpg


You may say that it looks pretty, but that's about all it did.  Notice how big
the battery is in relation to the car. Turns out, it was way too big.
Unfortunately, the car was significantly too heavy to move on its own.


*iframe*
https://drive.google.com/file/d/0Bw2eHK8Zx0mLWDV1Ui1NeHNuSE0/preview

So the heaviest part of the car was the battery pack powering the RasPi. After
getting some initial experiences it became clear that 10000 mAh is a lot of
more than sufficient power to run RasPi on, even with WiFi interface for
reasonable amount of time. Thus it was easiest to save weight by buying a way
lighter battery with less capactiy.

We were really lucky about the Pokemon Go craze that has rolled through the Netherlands
in early 2017, as it meant abundant supply of cheap mobile battery packs.
We quickly sourced a new one, and with the new battery, the car looked like this:

*img* 
img/car/green_ready_light.jpg

*img* 
img/car/green_ready_light_side.jpg

Unfortunately, the car was still way too weak to drive well with RasPi
strapped to its back. We needed a bigger car, but before I describe it,
I will show how we have put together image capture and streaming system.





*h3*
* Image capture and streaming *

Building this subsystem was a big loop of trial and error.
In the end we have settled for RPi Camera on hardware side and
amazing <b>picamera</b> Python module.

*img*
img/car/camera_holder.jpg

An interesting detail here is camera holder.  Filip works in <a
href="https://www.3dhubs.com"> 3D Hubs</a>, an Amsterdam based 3d printing
company. As a perk, team members are given significant 3D printing allowance,
which we used to 3d print case for RasPi camera in high quality. In this
technology they shoot plastic-paticle spray with laser so it solifidies.  This
technology results in very accurate and resilent products.

When it comes to the code side, at the beginning we have struggled a lot with
high latency:

*iframe* 
https://drive.google.com/file/d/0Bw2eHK8Zx0mLSHM5X1Btam5iWEk/preview

and then experimented some more:

*iframe*
https://drive.google.com/file/d/0Bw2eHK8Zx0mLOE93RTJHX0xpb0E/preview

and even more:

*iframe*
https://drive.google.com/file/d/0Bw2eHK8Zx0mLNDhkRFhjejZvMGs/preview

but quickly we have converged to a pretty resilent UDP based stream.  The code
version that has worked best for us in the end was version presented in <a
href="https://picamera.readthedocs.io/en/release-1.13/recipes2.html#rapid-capture-and-streaming">
Advanced Recipes in picamera docs</a>, but with UDP based connection. You can
see the code <a
href="https://github.com/ghostFaceKillah/mini-self-driving-car/blob/master/src/raspi/video.py">
here</a>. After this, we were consistently seeing good latency:

*img* 
img/car/measuring_latency_03.jpg

Measuring latency takes both hands.

** Show here the final picture that the car sees **

*img* 
img/car/ai_philosophy.jpg

What does the self-driving car see when it sees itself in the mirror?







*h4*
* Second car *

After the first car wouldn't move under it's own weight even with light battery
pack, it became clear we need a new one. In the local shopping mall, this
time in a proper toy shop we have found this car:

*img* 
img/car/black/black_vanilla.jpg

which is a bigger and slightly more advanced construction.
However, in this car we can find some interesting mechanical pieces
such as rear differential, and slightly more advanced front steering.
For now, I have resisted the temptation to tear it completely apart,
but I think I will come back to it after we move to bigger car.
The disassembly yielded a similar result for our project - three wire pairs,
one for battery, one for front steering one for back steering.

*img* 
img/car/black/black_disassembly.jpg

As we didn't want to disassemble the front steering, we were unsure
whether it will work in the simple, linear way. That is whether by just
applying voltage to front we could steer the car.
We tested it by hand:

*img* 
img/car/black/black_disassembly_01.jpg

and luckily it worked, which meant we could reuse the previous engine control
unit.  We used the opportunity to clean up the control module a bit and fit it
into slightly tighter package.

*img* 
img/car/black/black_engine_controller_clean.jpg

Overall, the ready product looked like this:

*img* 
img/car/black/black_iso_no_cam_2.jpg

After this we added above described high quality camera holder.

*img* 
img/car/black/black_ready_iso_01.jpg


The black car turned out to be quite fast:

*img*
img/car/black_is_fast.gif

And it was fun to just drive it around.

*iframe*
https://drive.google.com/file/d/0Bw2eHK8Zx0mLSm8xVFc4Rk5YR00/preview








*h4*
* Remote control *

Of course, driving by wire didn't cut it for us.


We have decided to stream the steering over WiFi. Another
viable alternative was Bluetooth.
First we have implemented event based controller that listens for keyboard
events on the controller server side. If it catches any, it sends a message
over TCP socket to the car to update the remote state.

On the testing bench, it has worked just fine:

*iframe*
https://drive.google.com/file/d/0Bw2eHK8Zx0mLZzdFVnNIY3VGMHc/preview

But in practice, it was error-prone:

*img*
img/car/black_crashes.gif

The default WiFi dongle we have bought for RasPi was too small and had not so
strong reception.  It was easy to get into a spot where there were problems with
instant transfer of the data.  As the TCP retries sending the data until
succesful, by the time the information arrived onto the car, several new events
might have happened.  The car did not feel very responsive.

We partially resolved this issue by:
<ul>
<li> sending the state (spamming?) to the car every 10 ms
<li> allowing information loss by switching to UDP
</ul>
so in the new implementation, algorithm listens for keyboard events and
updates the state based on it and we send the state to the car all the
time over and over again. You can see the final code 
<a href="https://github.com/ghostFaceKillah/mini-self-driving-car/blob/master/src/pc/keyboard_state_sender.py">
here</a>
and
<a href="https://github.com/ghostFaceKillah/mini-self-driving-car/blob/master/src/raspi/steering.py">
here</a>.

These changes in code helped the reliablity a lot, but it still wasn't perfect. Still 
in places with weak wifi reception, the car didn't work pefectly.
We felt there is still room for improvement.
We tried the easiest thing there was and added stronger Wifi dongle.

*img*
img/car/new_wifi_two.jpg

We went for pentesting-style Alfa wifi dongle with reasonably big antenna.
Look how well the dongle sits between the back engine and wing.
It's almost as if we planned it to be constructed that way!

*img*
img/car/black/black_wifi_on_car.jpg

But we did not. It's just a bunch of zipties.


This concluded the hardware part for now.
This is how it looks when you (badly) drive the car around the track in the evening.

*iframe*
https://drive.google.com/file/d/0Bw2eHK8Zx0mLRGdUSXc3NnJjd1U/preview







*h4*
Machine learning

We have implemented ConvNet-based behavioral cloning system that
steers the car.
It is a variation of widely used Nvidia implementation.

The inspiration came from many places:
<ul> 
<li> Reading the original Nvidia papers
    <a href="https://arxiv.org/pdf/1704.07911.pdf">
    "Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car"
    </a>
    and
    <a href="https://arxiv.org/pdf/1604.07316v1.pdf">
    "End to End Learning for Self-Driving Cars"
    </a>
<li> Berkley <a href="http://rll.berkeley.edu/deeprlcourse/"> Deep reinforcement learning class </a>
<li> Udacity self-driving car nanodegree
<li> Reading Comma.ai paper <a href="https://github.com/commaai/research"> 
"Learning a Driving Simulator"</a>
</ul>


Different thing is that Nvidia has 3 cameras which enable to gather 3
trajectories in one goal, especially trajectories around the most optimal one.
This is pretty big problem in behavioral clonging

In our case it is kind ok, because we have less general problem (only so many
tracks you can build around house), the track we follow is clearer and has less
variations than real-life road lanes and we have significant variance over
consecutive runs (we have more runs over the same area of track with a lot more
variance than in the case of real life car).


You can see that it is not perfect, doesn't stay inside of the track, but wit
current controls it may be beyond possible.  Should be possible with car that
has more precise and possibly non-binary left-right steering.



** link to the paper **

Did a bit data gathering, 2 sessions of around hour.

** how we gathered data **

** how the data gathering screen looks like ** 

Started off with gathering one big track, around 10 meters
around the whole house.

Trained a neural net like in the Nvidia paper.

It didn't work.

Then decided to simplify the problem to one turn.
Gathered a lot of data another time.

Then noticed that horizon is not consitent around takes...

Corrected, gathered data again...

Still didn't work!

Annotated the data manually again to control for dynamic problems.

Still didn't work!

Cast to greyscale, cropped.

Implemented data augmentation based on brigthness and contrast.

It worked!

Somewhat... Doesn't stay perfectly inside of the track 
(everything happens so much faster compared to real life car)
Tested well only around one turn track.

*h4*
* Lessons *

<ul>
<li> be patient and persistent
<li> need a bit of money
</ul>




TODO:
* Machine learning
* final car pictures
* describe the cool steering system from first car
* flesh out plans for future
* Good picture for 3d printed camera holder




*h4*
* Plans for future *

<ul>
<li> Riding around a bigger track
<li> Lane detection, cross track error based PID controller
<li> Lidar & mapping
<li> Path planning
<li> LSD SLAM
</ul>
