*h2*
How to build a small self driving car?

My recent side project is a building small self driving car with 
my friend Filip.  See the code on 
<a href="https://github.com/ghostFaceKillah/mini-self-driving-car.git">
my github</a>.

There is fair amount of folks doing similar projects, there are even people who
race these things.  If it looks interesting, you could do it too - it is easier than 
ever and I'm sure you'd learn a lot.

*wide-div*
<img src="img/car/car_works_1.gif">
<img src="img/car/car_works_2.gif">
<img src="img/car/car_works_3.gif">



*h4*
* Idea *

The car works in the following way:
<ul>
  <li>
    On board of the car there is forward facing <b>RasPi camera</b>.
  <li>
    The <b> image stream</b> is sent to GPU computer, 
    which streams <b>steering commands</b> back:
    "go forward", "stop", "turn left", etc.
  <li>
    Steering commands are issued in one of two ways:
    <ul>
      <li>
        <b>Human input</b>, where you steer the car in racing video game fashion.
      </li>
      <li>
        <b> Self-driving algorithm</b>, which tries to guess what would a human do 
        based on current video frame.
      </li>
    </ul>
  </li>
</ul>

Below diagaram visualizes the flow of data:

*img*
img/car/how_it_works.jpg

To enable all of this, pieces of various software infrastructure and hardware
are necessary.

On the GPU server side we need:
<ul>
<li> <a href="https://github.com/ghostFaceKillah/mini-self-driving-car/blob/master/src/pc/image_getter.py">code</a> 
     that receives images from RasPi
<li> <a href="https://github.com/ghostFaceKillah/mini-self-driving-car/blob/master/src/pc/keyboard_state_sender.py">code</a> 
     that sends current steering state to the car
<li> <a href="https://github.com/ghostFaceKillah/mini-self-driving-car/blob/master/src/pc/neural_network_computer.py">code</a> 
     that executes neural network to obtain autonomous steering
<li> <a href="https://github.com/ghostFaceKillah/mini-self-driving-car/blob/master/src/pc/pygame_driver.py">code</a> 
     that displays user the "steering game" and collects steering
<li> <a href="https://github.com/ghostFaceKillah/mini-self-driving-car/blob/master/src/pc/state_saver.py">code</a> 
     that collects human input pairs and saves them to the drive
</ul>
Each of the above is <b>multiprocessing.Process</b> and they share a state
which is <b>multiprocessing.Namespace</b>.
On the RasPi side we need:
<ul>
<li> <a href="https://github.com/ghostFaceKillah/mini-self-driving-car/blob/master/src/raspi/steering.py">code</a> that receives steering commands and executes them
<li> <a href="https://github.com/ghostFaceKillah/mini-self-driving-car/blob/master/src/raspi/video.py">code</a> that captures video stream and puts it on the wire
</ul>


On the hardware side there are following elements:
<ul>
<li> Raspberry Pi
<li> RasPi battery pack
<li> RasPi compatibile camera
<li> RC Car
<li> RC car engines battery pack
<li> Engine controller to control RC car
<li> Wifi interface (turns out it needs to be strong)
<li> Ways to keep all of this together
</ul>

In the text below I will explain how we figured out
how to combine all of them into a small self driving car,
and hopefully convey that it was a lot of fun.
And perhaps we have even learned a little.


*h4* 
* Before the first car *

The whole story started when I finally had time to learn some electronics.
I have always wanted to understand it, as it delivers such exceptional
amounts of value and - in my book - is as close to magic as it gets.

There are so many <a href="https://artofelectronics.net">cool books about electonics</a>,
<a href="http://lcamtuf.coredump.cx/electronics/">online write-ups</a>,
<a href="https://www.youtube.com/user/eaterbc">youtube videos</a>,
<a href="http://www.falstad.com/circuit/">electronics simulators</a>,
but what I found most practical for the car project was this book targetted to young teens:
<a href="https://www.amazon.com/Make-Electronics-Discovery-Charles-Platt/dp/0596153740">

*img*
img/car/book-make-electronics.jpg

</a>
It is so good:
<ul>
<li> has light-hearted vibe about it,
<li> teaches you the practical basics via small projects:
  <ul>
  <li>how to solder,
  <li> how to use multimeter,
  </ul>
<li> is exceptionally easy-to-read.
</ul>
It lacks the proper theory build-up, so you will have to read up on that
somewhere else if you are interested, but for the car project it should be
sufficient.  After some lazy evenings with soldering iron I was able to reverse
engineer and debug basic electronic circuits such as this "astable
multivibrator", to use the proper name:

*img*
img/car/blinkenlights.gif


*h3*
* The first car era *

Armed with knowledge of rudiments of electronics, it was time to reverse
engineer how an RC car works.  As I generally had little idea what I was doing,
I have decided to get cheapest one I could find so I wouldn't be too wasteful
if I broke it while disassembling it.  The cheapest one I could find was this
little marvel:

*img*
img/car/vanilla_green.jpg

which I have bought in Dutch bargain store Action.nl, new, for below 5 euro.

Deconstructing it, you just cannot help wondering how 
 simple and elegantly designed it is. 
It truly amazes me that for under 5 Euro per unit
people are able to design, fabricate, package, ship 
and sell this toy.
It is possible mostly due to amusingly "to the point", "brutalist"
design, where most of elements are right on their tolerances
and wanted effects are achieved in the simplest possible way.
This approach is best showcased by the design of the front wheels
steering system, which I will describe below.

Coming back to the self-driving project, when we pop the body
of the frame, we see this:

*img*
img/car/green_basic_deconstruct.jpg

Luckily for our project, 
this construction can be adapted to our needs in a straightforward fashion.
In the center we see the logic PCB.
There are three pairs of red & black wires attached to the PCB:
<ul>
<li> a pair feeding power from the battery pack
<li> another pair sending power to the back motor, which movese car forward and backward
<li> one more pair sending power to the front wheel steering system
</ul>

When logic board receives radio inputs, it (more-or-less) connects
the battery wires with the engine wires and the car moves.

*img*
img/car/green/green_pcb_closeup.jpg

You can see for yourself - if you directly connect the pair from the battery to
the pair coming from one of engines (front or back engine), you see that the
wheels move in wanted directions.  So if we substitute the current logic board
with our custom RasPi based logic board that controls connection between
battery and engines based on instructions that we control by code, we would be
able to drive-by-code.

*h4* 
Warning!

As Raspberry Pi has some General Purpose Input/Output (GPIO) pins
it is tempting to try to hook up the engine directly to RasPi.
This is a very bad idea, which will destroy your RasPi, by running too much power
through it.

Thus, we need to implement a hardware engine controller which based
on signals from Raspi will drive the motors.
You can buy a ready motor controller or be thrifty, and implement own.
As conceptually the wanted mechanism of action sounds like transistor - 
"based on small electricty turn on flow of bigger electricty" -
I have decided to build my own engine controller.
Following
<a href="https://business.tutsplus.com/tutorials/controlling-dc-motors-using-python-with-a-raspberry-pi--cms-20051">
this tutorial
</a>
produced an engine controller based on L293D chip.

*img*
img/car/green_engine_controller.jpg

The tutorial worked - at least partially. I was able to steer the front wheels
and move the car forward, but there was no reverse gear.

*iframe*
https://drive.google.com/file/d/0Bw2eHK8Zx0mLZ0ZxaVQ3UDdhNGM/preview

Turns out the method shown in the above tutorial couldn't spin second engine
in the reverse direction.
After consulting the 
<a href="http://www.ti.com/lit/ds/symlink/l293.pdf">
chip's factsheet
</a>
I have come up with another way of hooking up L293D, which fixed the bug. 
Now I could control car completely from Python code running on RasPi.

Now, as I had a car that can be controlled by keyboard attached to RasPi, it
was time to make the control remote. I have connected a basic small WiFi dongle
to Raspberry and implemented a pair of Python scripts:
<ul>
<li> Server-side script capturing keyboard events and sending control
events based on them to RasPi,
<li> RasPi-side script listening for those events on TCP socket and based on them
steering the L293D and in turn, the car.
</ul>


*h4*
* Steering mechanism in first car *

Now I will describe how steering is implemented mechanically 
in the first RC car we have been working with. 

Let me remark here that I am impressed by how simple this mechanism is.
Perhaps the reason I am so excited about it is because I have only a vague
idea about how mechanical design works, so I am easy to impress, and perhaps
stuff like this gets done every day, who knows.  But I see an element of playful
master in it and I can't help feeling happy about it.


The mechanism is hidden if you look from the front

*img*
img/car/steering/green_iso_front_view.jpg

and similarily remains hidden if you look at it from the top.

*img*
img/car/steering/green_top_view.jpg

It is only after removing the top cover that we see the following mechanism
(I have removed all the plastic elements to help visibility):

*img*
img/car/steering/green_side_front_engine.jpg

Seen from the top:

*img*
img/car/steering/green_engine_top.jpg

And here comes the trick:

On top of shaft there is a pinion. The pinion is just slid on shaft, without
any kind of glue.  The size and tightness of the pinion are chosen in in a way
that when the engine starts moving, the friction between shaft and pinion is
just enough to move the half-circle rack.

*img*
img/car/steering/green_front_engine_detail.jpg


But when the half-circle rack hits
the maximum extension point, the friction breaks and the shaft spins around
inside pinion.  Is there a cheaper trick (speaking both symbolically and
literally)?  How ingenious!

*img*
img/car/steering/green_steering_mechanism_action.gif

Below the semicircular rack there is a spring that pulls the wheels back
to neutral position:

*img*
img/car/steering/green_top_view_wheels_coming_off.jpg

You can see it better in this close-up:

*img*
img/car/steering/green_steering_insides_2.jpg

A small plastic element is connected to wheels. It has a small peg
that sits in the middle of the spring, so the spring an act on it:

*img*
img/car/steering/green_detached_wheels.jpg

So as you can see just a collection of as simple as possible 
elements each doing its job well. Everything is super cheap,
but the materials and their shapes are chosen in a smart
way that makes it all work together well. So cool!

That wraps up the description of the "brutalist"
steering mechanism.  Back to self driving!





*h4*
* Hardware * 

When we had already the "drive by keyboard attached to RasPi by cable"
capability, we wanted to test remote control of the car.
In order to do that, we needed to power the RPi.
Not sure about how much power it needs, we have bought a pretty big power
bank for mobile phone with electric charge of 10.000 mAh, weighting around 400 g.

I am huge fan of American automotive show 
<a href="https://www.youtube.com/watch?v=t8zYpmoV0qE&list=PL12C0C916CECEA3BC">
Roadkill</a>,
in which two guys, mostly by themselves, fix extremely clapped out classic
American cars, usually on the road / in WalMart parking lot / in the junkyard.
Typically they take some rotten engine-less chassis and put in a
cheapest-they-can-find V8 into them.  Needless to say, they are masters of
using zip-ties.

Inspired by Freiburger and Finnegan experiences, I have relied on ziptie engineering
in order to combine the collection of parts into one coherrent vehicle
that will move together. The end product looked like this:

*img* 
img/car/green/green_heavy_iso_03.jpg

*img* 
img/car/green/green_heavy_iso_02.jpg

*img* 
img/car/green_ready_front.jpg

*img* 
img/car/green_ready_top.jpg


You may say that it looks pretty, but that's about all it did.  Notice how big
the battery is in relation to the car. Turns out, it was way too big.
Unfortunately, the car was significantly too heavy to move on its own.


*iframe*
https://drive.google.com/file/d/0Bw2eHK8Zx0mLWDV1Ui1NeHNuSE0/preview

So the heaviest part of the car was the battery pack powering the RasPi. After
getting some initial experiences it became clear that 10000 mAh is a lot of
more than sufficient power to run RasPi on, even with WiFi interface for
reasonable amount of time. Thus it was easiest to save weight by buying a way
lighter battery with less capactiy.

We were really lucky about the Pokemon Go craze that has rolled through the Netherlands
in early 2017, as it meant abundant supply of cheap mobile battery packs.
We quickly sourced a new one, and with the new battery, the car looked like this:

*img* 
img/car/green_ready_light.jpg

*img* 
img/car/green_ready_light_side.jpg

Unfortunately, the car was still way too weak to drive well with RasPi
strapped to its back. We needed a bigger car, but before I describe it,
I will show how we have put together image capture and streaming system.





*h3*
* Image capture and streaming *

Building this subsystem was a big loop of trial and error.
In the end we have settled for RPi Camera on hardware side and
amazing <b>picamera</b> Python module.

*img*
img/car/camera_holder.jpg

An interesting detail here is camera holder.  Filip works in <a
href="https://www.3dhubs.com"> 3D Hubs</a>, an Amsterdam based 3d printing
company. As a perk, team members are given significant 3D printing allowance,
which we used to 3d print case for RasPi camera in high quality. In this
technology they shoot plastic-paticle spray with laser so it solifidies.  This
technology results in very accurate and resilent products.

When it comes to the code side, at the beginning we have struggled a lot with
high latency:

*iframe* 
https://drive.google.com/file/d/0Bw2eHK8Zx0mLSHM5X1Btam5iWEk/preview

and then experimented some more:

*iframe*
https://drive.google.com/file/d/0Bw2eHK8Zx0mLOE93RTJHX0xpb0E/preview

and even more:

*iframe*
https://drive.google.com/file/d/0Bw2eHK8Zx0mLNDhkRFhjejZvMGs/preview

but quickly we have converged to a pretty resilent UDP based stream.  The code
version that has worked best for us in the end was version presented in <a
href="https://picamera.readthedocs.io/en/release-1.13/recipes2.html#rapid-capture-and-streaming">
Advanced Recipes in picamera docs</a>, but with UDP based connection. You can
see the code <a
href="https://github.com/ghostFaceKillah/mini-self-driving-car/blob/master/src/raspi/video.py">
here</a>. After this, we were consistently seeing good latency:

*img* 
img/car/measuring_latency_03.jpg

Measuring latency takes both hands.

** Show here the final picture that the car sees **

*img* 
img/car/ai_philosophy.jpg

What does the self-driving car see when it sees itself in the mirror?







*h4*
* Second car *

After the first car wouldn't move under it's own weight even with light battery
pack, it became clear we need a new one. In the local shopping mall, this
time in a proper toy shop we have found this car:

*img* 
img/car/black/black_vanilla.jpg

which is a bigger and slightly more advanced construction.
However, in this car we can find some interesting mechanical pieces
such as rear differential, and slightly more advanced front steering.
For now, I have resisted the temptation to tear it completely apart,
but I think I will come back to it after we move to bigger car.
The disassembly yielded a similar result for our project - three wire pairs,
one for battery, one for front steering one for back steering.

*img* 
img/car/black/black_disassembly.jpg

As we didn't want to disassemble the front steering, we were unsure
whether it will work in the simple, linear way. That is whether by just
applying voltage to front we could steer the car.
We tested it by hand:

*img* 
img/car/black/black_disassembly_01.jpg

and luckily it worked, which meant we could reuse the previous engine control
unit.  We used the opportunity to clean up the control module a bit and fit it
into slightly tighter package.

*img* 
img/car/black/black_engine_controller_clean.jpg

Overall, the ready product looked like this:

*img* 
img/car/black/black_iso_no_cam_2.jpg

After this we added above described high quality camera holder.

*img* 
img/car/black/black_ready_iso_01.jpg


The black car turned out to be quite fast:

*img*
img/car/black_is_fast.gif

And it was fun to just drive it around.

*iframe*
https://drive.google.com/file/d/0Bw2eHK8Zx0mLSm8xVFc4Rk5YR00/preview








*h4*
* Remote control *

Of course, driving by wire didn't cut it for us.


We have decided to stream the steering over WiFi. Another
viable alternative was Bluetooth.
First we have implemented event based controller that listens for keyboard
events on the controller server side. If it catches any, it sends a message
over TCP socket to the car to update the remote state.

On the testing bench, it has worked just fine:

*iframe*
https://drive.google.com/file/d/0Bw2eHK8Zx0mLZzdFVnNIY3VGMHc/preview

But in practice, it was error-prone:

*img*
img/car/black_crashes.gif

The default WiFi dongle we have bought for RasPi was too small and had not so
strong reception.  It was easy to get into a spot where there were problems with
instant transfer of the data.  As the TCP retries sending the data until
succesful, by the time the information arrived onto the car, several new events
might have happened.  The car did not feel very responsive.

We partially resolved this issue by:
<ul>
<li> sending the state (spamming?) to the car every 10 ms
<li> allowing information loss by switching to UDP
</ul>
so in the new implementation, algorithm listens for keyboard events and
updates the state based on it and we send the state to the car all the
time over and over again. You can see the final code 
<a href="https://github.com/ghostFaceKillah/mini-self-driving-car/blob/master/src/pc/keyboard_state_sender.py">
here</a>
and
<a href="https://github.com/ghostFaceKillah/mini-self-driving-car/blob/master/src/raspi/steering.py">
here</a>.

These changes in code helped the reliablity a lot, but it still wasn't perfect. Still 
in places with weak wifi reception, the car didn't work pefectly.
We felt there is still room for improvement.
We tried the easiest thing there was and added stronger Wifi dongle.

*img*
img/car/new_wifi_two.jpg

We went for pentesting-style Alfa wifi dongle with reasonably big antenna.
Look how well the dongle sits between the back engine and wing.
It's almost as if we planned it to be constructed that way!

*img*
img/car/black/black_wifi_on_car.jpg

But we did not. It's just a bunch of zipties.


This concluded the hardware part for now.
This is how it looks when you (badly) drive the car around the track in the evening.

*iframe*
https://drive.google.com/file/d/0Bw2eHK8Zx0mLRGdUSXc3NnJjd1U/preview







*h4*
Machine learning

We have implemented ConvNet-based behavioral cloning system that
steers the car.
It is a variation of a widely used Nvidia implementation.
See the full code 
<a href="https://github.com/ghostFaceKillah/mini-self-driving-car/blob/master/src/ml/train_model.py">
here</a>.

The inspiration came from many places:
<ul> 
<li> Reading the original Nvidia papers
    <a href="https://arxiv.org/pdf/1704.07911.pdf">
    "Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car"
    </a>
    and
    <a href="https://arxiv.org/pdf/1604.07316v1.pdf">
    "End to End Learning for Self-Driving Cars"
    </a>
<li> Berkley <a href="http://rll.berkeley.edu/deeprlcourse/"> Deep reinforcement learning class </a>
<li> Udacity self-driving car nanodegree
<li> Reading Comma.ai paper <a href="https://github.com/commaai/research"> 
"Learning a Driving Simulator"</a>
</ul>

As shown in the diagram at the top of the blog post, based on a dataset of
pairs <b><i> (image, steering) </b></i> sampled from a human driving record, we
learn to predict <b><i> steering </b></i> from <b><i> image from camera
</b></i>.  This is done by using a convolutional neural network.

There are good resources galore on deep learning / neural nets online, so I won't go into
detail how this was implemented. Instead, I will focus on explaining 
differences between our and Nvidia's approches.

<b>First of all</b>, in our case the steering is binary.  In a typical real-world
self-driving car implementation you would gain control over steer-by-wire
system done by car manufacturer.  This usually means you gain control
over pretty precise actuators that allow you to set the steering angle with
consistency and precision.  This was not the case in our model. Currently we
only have three steering states - steer full left, steer full right or no
steering.  Additionally, when returning from full steer to the neutral
position, the mechanism is often not completely accurate and returns only to
somewhat-neutral position with bias to one side that is noticeable when the car
goes fast. Check the driving video linked above and compare how fast everything
happens compared to driving a normal car.

<b>Secondly</b>, in our implementation we only use one camera, where Nvidia's
implementation uses three cameras that are generally forward facing,
but at slightly different angles.
This trick is done to get rid of a problem specific to behavioral learning.
The problem is overrepresentation of idealized trajectories in the training data.
If you consider a person driving a car on a road, even on a closed-off private road,
the samples from the car going off-road, or near-crashing and recovering in the
last moment are extremely rare compared to calm driving in the middle of the world.
However, for the algorithm both cases are as important as the other.
The self driving systems needs to know what to do when it starts going away from middle
of the track.

*img*
img/car/behavioral_cloning_problem.jpg

<i>I have copy-pasted this image from
<a href="http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_2_behavior_cloning.pdf">slides</a>
of Sergey Levine for one of the beginning lectures of
<a href="http://rll.berkeley.edu/deeprlcourse/">
Berkeley CS294-112: Deep Reinforcement Learning</a>.
- an incredibely high-quality online course generously offered for free by this university.</i>

You can see in this image that during the training, we have explored the black trajectory.
Due to inherent randomness, we are slightly diverging from the most well-known
trajectory, for example the car steers a bit more to the right then usually.
The correct response would be to correct and go back to the middle of the
track, but there is no data like this in the training dataset (or there is
but it is not sufficiently frequent to be represented in the machine learning).

The solution to this is to build a distribution of cost (and therefore, of
wanted behaviours) around the optimal trajectory:

*img*
img/car/distribution_over_trajectories.jpg

<i>Image, again courtesy to UC Berkeley, Prof Sergey Levine</i>.

There are several ways one could go around producing such a distribution.

One could try to <b>gather a lot more data</b>, especially taking care to sample
trajectories that are away from the optimal trajectory. In the case of the car it
could be turning off recording, steering slightly off the center of the road,
turning recording back on, correcting the steering back to the center. Another
idea would be to drive over the same piece of road many times, taking care to
drive a bit to the left, than a bit to the right, then more to the right etc.
As you can see, these solutions are doable, but not extremely practical, so it
is not always there is an easy way to just gather the data.

One could consider the trick that Nvidia has done. They have set up two cameras at slight
angles to left and right
in addition to the central forward facing one.
Then they correct for this in the steering commands that are recorded
as the pair for that sideways facing image.
So for example if they are driving straight forward, the right facing
camera will record a picture as if the car was going to the right of the
road, and a steering correction of the same angle to the left would be added
to the current (fully straight steering).
This will tell help the machine learning algorithm understand that 
it needs to steer left when it sees this image.

*img*
img/car/steering_adjustment.jpg


In our case the problem is less severe than in the case of the real self-driving car.
This is because:
<ul> 
<li>
we have less general problem. The variety of tracks that we can build around the house 
is significantly smaller than the variety of real roads, especially when you 
take into consideration the spectrum of possible lighting conditions.
Our track is white pieces of paper, which is very clearly visible and 
similar to itself under a variety of conditions.
<li> the tracks that we train on are shorter so we can run the car
over each piece of road hundreds of times in a day or so. Imagine doing that 
for each piece of road in Europe or US.
<li> We have significant variations of runs due to lack of precision of our
actuators. Driving in the small car world is a lot more dynamic,
the turns are a lot sharper and we can afford to crash for free.
Check out driving video above to see how quick everything happens.
</ul>

We have also added some more layers to the NVIDIA architecture, but I don't
think it should make a big difference (as the structure of the data we have
is relatively simple).

One more difference is purely architectural - NVIDIA has run the deep learning
directly on the car, using cutting edge (but super expensive)
<a href="http://www.nvidia.com/object/drive-px.html">
NVIDIA DRIVE PX</a> computer for autonomous driving.
NVIDIA claims that they got 30 fps, and we got 25 fps, so I would say not too bad.

In general the system works well on the track it was trained on, but the
performance doesn't generalize to a more complicated, tighter track. We will run
some experiments about training on the new track soon. My guess the 
performance should be improving steadily with amount of training data gathered.

You can see that even on the vanilla one-turn track the driving is not perfect,
doesn't stay inside of the track, but with the current controls it may be
beyond possible. It should be possible with a car that has more precise, preferably
non-binary left-right steering.

*h3* 
* Machine learning development story * 

From the beginning of the development, we were pretty sure that our approach
should work - we knew it has been applied sucessfully in real life applications
and very similar project but inside of simulator was part of Udacity
self-driving car course. Still, getting it right required some degree of patience
and creative tweaks. 

After the hardware construction & testing phase, we had a real-time, low-latency system where you
drive the car around track, based solely on the video input.

The final user interface looks like this:

*img*
img/car/ux.jpg

Here you can see a previous iteration of it being tested carefully by another friend.

*iframe*
https://drive.google.com/file/d/0Bw2eHK8Zx0mLWmFPc2FkLUp3dU0/preview

So we started off with gathering one big track, around 10 meters around the
whole house.  While you drive, everything you do gets

We have trained a neural net same as in the Nvidia paper.  It didn't work!  And
by didn't work we don't mean running it once and declaring failure.  In
general, we tried to make at least three optimization runs for each setting of
architecutre / metaparameters to be sure to take out effect of random
initialization.

After the initial attempt didn't work, we decided to simplify the problem to
just one turn.  We have gathered a lot of data another time - more or less 2
sessions of 2 hours each.  The raw collected pictures look like this:

*img*
img/car/training_data.jpg

After training the network we have found that again it doesn't work.  We had a
brainstorming session we decided that most effective would be to implement a
manual data adjuster.  We were thinking that perhaps there was too much error
in our training data (that we were driving too much outside of the lane as
things were happening too fast, that there was overrepresentation of start of
track and end of track with 0 velocity but nonzero steering).  We have
implemented some additional infrastructure to help debugging this kind of data
errors - a data viewer / editor, capability to turn recording on and off in
data collection, a dataset combiner.  Then we manually went through all of the
data and corrected what we thought were inconsistencies.  This also didn't
work...

** Show a recording of how it doesn't work **

Then we spent some even more time looking at the data and only then we have noticed
that horizon is located differently in different parts of the dataset.
That meant that our camera was moving around too much, introducing
significant noise in the data.
We have fixed the camera placement, added a guiding line in the driver UI and
collected more data. Still didn't work!

Then we had the breakthrough, which was noticing that we should cast to grayscale
and implement brightness and contrast adjustments. We do it to correct for 
differing lighting conditions. An example of our final augmentation pipeline is
visible below:


*img*
img/car/img_aug_2.jpg

Than it finally worked!  Somewhat... The car doesn't stay perfectly inside of the track 
(everything happens so much faster compared to real life car)
We only tested the whole project well only around one turn track.

** How it doesn't work now video **

We can squeeze out / cheat a run that doesn't exit track
 around a more curvy, tighter track, but it doesn't count. But still
it shows some degree of generalization.
We think that if you just gather more data with the current architecture,
it should work.


*h4*
* Lessons *

<ul>
<li> be patient and persistent
<li> need a bit of money
</ul>




TODO:
* Machine learning
* final car pictures
* flesh out plans for future



*h4*
* Plans for future *

<ul>
<li> Riding around a bigger track / more complicated track
<li> Depth nets
<li> Lane detection, cross track error based PID controller
<li> Lidar & mapping
<li> Path planning
<li> LSD SLAM
</ul>
