*h1*
Expert-augmented actor-critic 

*h3*
for Vizdoom and Montezuma's Revenge

We propose an actor-critic algorithm reinforcement learning algorithm, which
can additionally utilise expert trajectories. The algorithm is evaluted on two
environments with sparse rewards: Montezuma’s Revenge and Vizdoom. On
Montezuma’s Revenge, our agent achieves qualitatively strong results,
consistently scoring results above 8000 points and in some experiments solving
the first world. In the case of Vizdoom, the agent learns to navigate a
complicated maze in a scenario which is too difficult to be solved by
model-free algorithms not augmented by expert data.


*h3*
Introduction

Deep reinforcement learning has shown impressive results in simulated
environments. However, as the cost of random exploration increases rapidly with
the distance of rewards, current approaches often fail when rewards are sparse.
This inhibits using reinforcement learning methods in real-world applications.
Take as an example robotics, where in many cases rewards are calculated once a
task is completed and thus are binary and sparse. The situation is aggravated
if no simulation is available, making sample efficiency a key factor of
success.

One way to improve the efficiency of exploration is to utilize expert data. The
standard behavioral cloning often suffers from compounding errors when drifting
away from the supervisor’s demonstra- tions. While this can be mitigated by
iterative methods like DAgger, the cost is cumbersome data collection process.
In recent [1] authors analyze performance of behavioral cloning on Atari 2600
games. In the challenging example of Montezuma’s Revenge their method reaches
on average only 575 points despite being trained on demonstration trajectories
that score 30 000 points.

Our approach is based on the Actor-Critic using Kronecker-Factored Trust Region
(ACKTR) algo- rithm [3]. This algorithm uses natural gradient techniques to
accelerate the gradient ascent optimization by changing parameters in the
direction that minimizes the loss with respect to small step in the distri-
bution of network output (in our case policy), as opposed to small step in the
parameter space metric. Natural gradient approaches proved to be successful in
increasing speed and stability of learning. We modify ACKTR so that it utilizes
expert data. We believe that expert data guides agent’s exploration. In our
evalutation, on Montezuma’s Revenge and Vizdoom environments this substantially
accelerates the learning process.








*h3*
Expert-augmented ACKTR

*img*
img/expert/main-formula.png

*wide-div*
<img src="img/expert/model-pro.png">

*wide-div*
$$
 L_t(\theta) =
 \mathbb{E}_{\pi_{\theta}} \left[
    \underbrace{
        -\text{adv}_t
         \log \pi_\theta (a_t | s_t)
        + \frac{1}{2} (R_t - V_\theta(s_t))^2
    }_{ L^{\text{A2C}}_t(\theta) }
    -
    \lambda_{\text{expert}}
    \underbrace{
       \text{adv}^{\text{expert}}_t
        \log \pi_\theta (a^{\text{expert}}_t | s^{\text{expert}}_t)
    }_{L^{\text{expert}}_t(\theta)}
\right]
$$

*img*
wow.svg



The expert data is sampled from a fixed dataset of rollouts achieving high
rewards. We consider 3 variants of the expert advantage estimators:

<ul>
    <li>
        reward: $ \text{adv}^{\text{expert}}_t =
                \sum_{s\geq 0}  \gamma^s r_{t+s}^{\text{expert}} $\; 
    </li>
    <li>
        actor-critic:  
            $\text{adv}^{\text{expert}}_t = \left[ \sum_{s\geq 0}
            \gamma^s r_{t+s}^{\text{expert}}-
             V_{\theta}(s_t) \right]_{+} $, where $[x]_+ =
             max(x, 0)$\;
    </li>
    <li>
        simple: $ \text{adv}^{\text{expert}}_t= 1 $.  
    </li>
</ul>


Expert data is not used in ACKTR estimation of inverse Fisher, but still during
the Kroncker optimization step $g_{\text{expert}}$ is projected to the natural
gradient direction.




*h3*
Algo listing


 Inputs: Parameter vector $\theta$; <br>
 Dataset of expert transitions 
 $ (s^{\text{expert}}_t, a^{\text{expert}}_t,
    s^{\text{expert}}_{t+1}, r^{\text{expert}}_t) $  <br>
 for $iteration \leftarrow 1$ \KwTo max steps <br> {
   \For{$t \leftarrow 1$ \KwTo T}{
      Perform action $a_t$ according to $\pi_\theta(a|s_t)$ \\
      Receive reward $r_t$ and new state $s_{t+1}$
   }
   \For{$t \leftarrow 1$ \KwTo $T$}{
      Compute discounted future reward: $\hat{R}_t = r_t + \gamma r_{t+1} + ... + \gamma^{T-t+1}r_{T-1} + \gamma^{T-t}V_{\theta}(s_t)$ \\
      Compute advantage: $\text{adv}_t = \hat{R}_t - V_{\theta}(s_t)$
   }
   Compute A2C loss gradient
   $g_{A2C} =  \nabla_\theta \sum_{t = 1}^T 
     \text{adv}_t \log \pi_\theta(a_t | s_t)  +
     \frac{1}{2} (\hat{R}_t - V_{\theta})^2
   $ \\
   Sample mini batch of $k$ expert state-action pairs \\
   Compute expert advantage estimate $\text{adv}^{\text{expert}}_t$ for each state-action pair. \\
   Compute expert loss gradient
   $g_{\text{expert}} = \nabla_\theta \frac{1}{k} \sum_{i=1}^{k}
    \text{adv}^{\text{expert}}_i \log \pi_\theta(a^{\text{expert}}_i | s^{expert}_i)
   $ \\
   Update ACKTR inverse Fisher estimate. \\
   Plug in gradient $g = g_{A2C} + \lambda_{\text{expert}}g_{\text{expert}}$ into ACKTR Kronecker optimizer.
 }
 \caption{Expert-augmented ACTKR}
 \label{listing}
